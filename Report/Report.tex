\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry} % Add geometry package here
\geometry{margin=1.2in} % Set the margin size here
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{float}
\usepackage[backend=biber]{biblatex}

\addbibresource{sources.bib}

\pgfplotsset{compat=1.16}

\graphicspath{ {./images/} }

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    pdfpagemode=FullScreen,
}

\definecolor{code-gray}{RGB}{220, 220, 220}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  showstringspaces=false,
  backgroundcolor=\color{code-gray}
}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}


\begin{document}

\begin{titlepage}
  \begin{center}
      \vspace*{1cm}

      \LARGE
      \textbf{Diffusion Models in Depth: From a Theoretical and a Practical Perspective}

      \vspace{1.5cm}

      \Large
      \textbf{Gregory Igor Sedykh}
      \vspace{0.8cm}

      \normalsize
      June 28th 2024

      \vfill

      \includegraphics[width=0.4\textwidth]{images/informatics_en.png} \\

      Bachelor thesis for the degree of Bachelor of Science in Computer Science \\
      Supervised by Prof. St√©phane Marchand-Maillet     

      \vspace{0.8cm}
    
           
           
  \end{center}
\end{titlepage}

\pagenumbering{roman}
\newpage
\section*{Abstract}

\newpage
\tableofcontents

\pagenumbering{arabic}

%% ----------------------- Content goes here ----------------------- %%

\newpage
\section{Introduction}

Diffusion models have gained widespread popularity since 2020, as models such as DALL-E, Stable Diffusion and Midjourney have proven to be capable of generating high-quality images given a text prompt. Furthermore, OpenAI's recent announcement of Sora has shown that diffusion models have now also become highly capable of generating minute long high-definition videos from a text prompt. \cite{videoworldsimulators2024}
\\\\
These models date back to 2015, where the idea of a diffusion model appeared, based on diffusion processes used in thermodynamics. \cite{sohldickstein2015deep} \\
Denoising Diffusion Probabilistic Models (DDPMs) were a development of the original diffusion probabilistic model introduced in 2015. \cite{ho2020denoising} \\
Subsequently, OpenAI improved upon the original DDPMs which did not have ideal log likelihoods \cite{ho2020denoising} while also using less forward passes and therefore speeding up the sampling process. \cite{nichol2021improved} \\
The most recent progress done by OpenAI has allowed their diffusion models to obtain better metrics and better sample quality than Generative Adversarial Networks (GANs) which were previously considered the state-of-the-art in image generation. \cite{dhariwal2021diffusion}
\\\\
The fairly recent apparition of diffusion models means not only that there is still a lot to be discovered about them, but also that progress is being made rapidly. \\
The theory behind diffusion models was mainly founded when Ho et al. \cite{ho2020denoising} introduced their DDPMs in 2020, but many improvements have been made upon their work since then. \\ 
To understand what these models are and how they work, it is crucial to understand how DDPMs were developed, what choices were made when developing them and why these choices were made, as well as why changes were made to the original model and how they were made.
\\\\
This report aims to provide an overview of the theory behind diffusion models, as well as a practical guide on how to implement a simple diffusion model using PyTorch, in order to compare what the theory shows us and what the practical implementation gives us.

\newpage
\section{Denoising Diffusion Probabilistic Models}

A \textbf{diffusion model} is a generative model that consists of two Markov chains, one forward and one reverse. \\
Given an input (e.g. an image), the forward process will destroy the image by gradually adding Gaussian noise at each step of the process. \cite{ho2020denoising} \\
The reverse process' objective is to "reverse" the forward process, starting from the noisy image and step-by-step, estimating the noise that was added to the image at each step and removing it until the first step is reached, where we should obtain the original image. \cite{ho2020denoising} \\

\subsection{Forward Process}

The forward process is a Markov process that starts from the original image $x_0$ and adds Gaussian noise during $T$ steps, where each step $t \in \left[1, T\right]$ has a size $\beta_t \in \{ \beta_1, ..., \beta_T \}$, which results in a more noisy image $x_t$. \cite{ho2020denoising}. \\
It is defined by $q$ which is a probability distribution that takes as input an image $x_{t}$ and outputs its probability.
\begin{align}
  q: \mathbb{R}^D \rightarrow [0, 1]
\end{align}
where $D$ is the data dimensionality (e.g. for a $64 \times 64$ RGB image, $D = 64 \times 64 \times 3$). \\
Each component $x_t^i \sim \mathbb{R}$ (pixel $i$ of the image $x_t$) and they are mutually independent since the covariance matrix ($\beta_t I$) is diagonal.
\\\\
Formally, we obtain:
\begin{align}
  q\left(x_{1},..., x_{T} | x_0\right) = \prod_{t = 1}^T{q\left(x_t | x_{t - 1}\right)}
\end{align}
where:
\begin{align}
  q\left(x_t | x_{t-1}\right) = \mathcal{N}\left(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I\right)
\end{align}
\\\\
Equation \ref{eq:2} gives us a single step forward: given the previous image $x_{t-1}$, we add noise from a Gaussian distribution with mean $\sqrt{1 - \beta_t}x_{t-1}$ and variance $\beta_t I$ to obtain the next image $x_t$. \\
Equation \ref{eq:1} gives us the full forward process from the original image $x_0$ to the final image $x_T$.
\\\\
The reparametrisation trick says that for a univariate Gaussian distribution where $z \sim p\left(z | x\right) = \mathcal{N}\left(\mu, \sigma^2\right)$, a valid reparametrisation would be $z = \mu + \sigma \epsilon$ where $\epsilon \sim \mathcal{N}\left(0, 1\right)$ is just noise. \cite{kingma2022autoencoding} \\
In this case, we can see that the image becomes more random because of the added Gaussian noise from $\epsilon_t$ at each step, since we are sampling:
\begin{align}
  x_t &= q(x_t | x_{t-1}) \\
  &= \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t I} \epsilon_t \qquad \epsilon_t \sim \mathcal{N}\left(0, I\right)
\end{align}
\\\\
It is important to note that as $T \rightarrow \infty$ and with a correct choice of $\beta_t$,  $x_T$ will become a sample of an isotropic Gaussian distribution ($\mathcal{N}\left(0, I\right)$). \cite{nichol2021improved} \cite{sohldickstein2015deep}. \\
This is important for the reverse process, as it will allow us to take a sample $x_T \sim \mathcal{N}\left(0, I\right)$ and reverse the forward process to obtain the original image $x_0$ (however this cannot be done so simply, as seen in section 3) \cite{nichol2021improved}.
\\\\
Ho et al. \cite{ho2020denoising} use the reparametrisation trick to be able to sample $x_t$ at any arbitrary step $t$ of the forward process.
\\\\
Let $\alpha_t = 1 - \beta_t$, then:
\begin{gather*}
  x_t \sim \mathcal{N}\left(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I\right) \\
  x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t I} \epsilon_{t-1} \qquad \epsilon_{t - 1} \sim \mathcal{N}\left(0, I\right) \\
  x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t - 1}
\end{gather*}
\\\\
From this, we can apply it again to $x_{t-1}$ and obtain:
\begin{gather*}
  x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t - 2}
\end{gather*}
\\\\
Therefore, we get:
\begin{gather*}
  x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t\left(1 - \alpha_{t-1}\right)} \epsilon_{t - 2} + \sqrt{1 - \alpha_t} \epsilon_{t - 1}
\end{gather*}
\\\\
We can write:
\begin{align}
  x_t &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \alpha_t\left(1 - \alpha_{t-1}\right)\right) + \sqrt{1 - \alpha_t} \epsilon_{t - 1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \alpha_t\left(1 - \alpha_{t-1}\right)\right) + \mathcal{N}\left(0, 1 - \alpha_t\right)
\end{align}
\\
The convolution of the two Gaussian distributions gives us a new Gaussian distribution with mean $\mu_1 + \mu_2$ and variance $\sigma_1^2 + \sigma_2^2$ (see Appendix A):
\begin{align}
  x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \left(\alpha_t\left(1 - \alpha_{t-1}\right) + 1 - \alpha_t\right)I\right)
\end{align}
\\
Which finally gives us:
\begin{gather*}
  x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon}_{t - 2}
\end{gather*}
\\\\
We can recursively apply this until $x_0$. \\
Let $\bar{\alpha}_t = \prod_{i=0}^{t}{\alpha_i}$:
\begin{gather}
  x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon}
\end{gather}
\\\\
This will give us a way to find a noisy image at step $t$ of the forward process, given the original image $x_0$ (we ignore the noise $\bar{\epsilon} \sim \mathcal{N}\left(0, I\right)$):
\begin{gather}
  q\left(x_t | x_0\right) = \mathcal{N}\left(\sqrt{\bar{\alpha}_t} x_0, \left(1 - \bar{\alpha}_t\right)I\right)
\end{gather}
\\
We now also know that $1 - \bar{\alpha}_t$ is the variance of the noise added at step $t$ of the forward process. \cite{nichol2021improved}
\\\\
Choosing the variances $\beta_t$ is an important step when developing the forward process. \\
Ho et al. \cite{ho2020denoising} chose to use a linear schedule starting from $\beta_1 = 10^{-4}$ and increasing linearly until $\beta_T = 0.02$ in order for them to be small compared to the pixel values that were in $\left[-1, 1\right]$. \cite{ho2020denoising} \\
However we will see in section 3 that this is not the best choice, as this destroyed the images too quickly closer to the end of the process while providing little to sample quality, which made the model be sub-optimal for $64 \times 64$ and $32 \times 32$ images. \cite{nichol2021improved}
\\\\
Finally, Ho et al. \cite{ho2020denoising} chose $T = 1000$ in order to be able to compare their model with Sohl et al.'s \cite{sohldickstein2015deep} model, which also used $T = 1000$ for most image experiments. \\
We will again see in section 3 that so many steps can make the sampling slow and that there are better choices. \cite{nichol2021improved}
\subsection{Reverse Process}

As mentioned previously, with a correct choice of $\beta_t$, $x_T$ will become a sample of an isotropic Gaussian distribution over $\mathbb{R}^D$ as $T \rightarrow \infty$. \cite{nichol2021improved, sohldickstein2015deep} \\
This should mean that we can take a sample $x_T \sim \mathcal{N}\left(0, I\right)$ and reverse the forward process to obtain the original image $x_0$. 
However, this means that we need to use the entire dataset in order to figure out $q\left(x_{t-1} | x_t\right)$, which in practice cannot be done. \cite{nichol2021improved} \\
Therefore, an estimation is found using a neural network. \cite{nichol2021improved}
\\\\
The reverse process is defined as follows \cite{ho2020denoising}:
\begin{gather}
  p_{\theta}\left(x_0, ..., x_T\right) = p\left(x_T\right) \: \prod_{t=1}^T p_{\theta}\left(x_{t-1} | x_t\right)
\end{gather}
where
\begin{gather}
  p_{\theta}\left(x_{t-1} | x_t\right) = \mathcal{N}\left(x_{t-1}; \mu_{\theta}\left(x_t, t\right), \Sigma_{\theta}\left(x_t, t\right)\right)
\end{gather}
\\\\
Equation \ref{eq:5} gives us the full reverse process, starting from the final image $x_T$ to the original image $x_0$. \\
Equation \ref{eq:6} gives us the reverse process at step $t$, where we estimate the noise that was added to the image at step $t$ of the forward process in order to find the image $x_{t-1}$. \\
Two parameters must be estimated to find the reverse process at step $t$: the mean $\mu_{\theta}\left(x_t, t\right)$ and the variance $\Sigma_{\theta}\left(x_t, t\right)$. \\
At first, Ho et al. \cite{ho2020denoising} used neural networks to estimate both the mean and the variance, but explain that estimating the variance was not necessary as there was little difference between the estimated variance $\sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}$ and the actual values $\sigma_t^2 = \beta_t$ \cite{ho2020denoising}. Thus they set $\Sigma_{\theta}\left(x_t, t\right) = \sigma_t^2 I$. \\
As for the mean, it is estimated using a neural network trained to optimise a variational lower bound ($VLB$) of the negative log likelihood $\mathbb{E}\left[- \log p_{\theta} \left(x_0\right)\right]$ \cite{ho2020denoising}

\subsubsection{Variational Lower Bound}
In order to find the negative log likelihood $\mathbb{E}\left[- \log p_{\theta} \left(x_0\right)\right]$, we would need to know $p_{\theta} (x_0)$ which means going through $T$ steps, which is computationally expensive. \\
Thus, Ho et al. \cite{ho2020denoising} use a variational lower bound ($VLB$ or more commonly known as $ELBO$) to estimate the negative log likelihood \cite{ho2020denoising,sohldickstein2015deep}.
The variational lower bound given by Dederik et al. \cite{kingma2022autoencoding} leads us to:
\begin{gather}
  VLB = \log p_{\theta}\left(x_0\right) - D_{KL}\left(q\left(x_{1:T}|x_0\right) \| p_{\theta}\left(x_{1:T}|x_0\right)\right)
\end{gather}
Since we need to minimise the loss with the negative log likelihood, we can write:
\begin{gather}
  - VLB = - \log p_{\theta}\left(x_0\right) + D_{KL}\left(q\left(x_{1:T}|x_0\right) \| p_{\theta}\left(x_{1:T}|x_0\right)\right)
\end{gather}
We know that the Kullback-Leibler divergence $D_{KL}$ is non-negative, which means that we can write:
\begin{align}
  - \log p_{\theta}\left(x_0\right) &\leq - \log p_{\theta}\left(x_0\right) + D_{KL}\left(q\left(x_{1:T}|x_0\right) \| p_{\theta}\left(x_{1:T}|x_0\right)\right) \\[10pt]
  &= - \log p_{\theta}\left(x_0\right) + \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{p_{\theta}\left(x_{1:T}|x_0\right)}\right] \\[10pt]
  &= - \log p_{\theta}\left(x_0\right) + \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{\frac{p_{\theta}\left(x_0, x_{1:T}\right)}{p_{\theta}\left(x_0\right)}}\right] \\[10pt]
  &= - \log p_{\theta}\left(x_0\right) + \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{\frac{p_{\theta}\left(x_{0:T}\right)}{p_{\theta}\left(x_0\right)}}\right] \\[10pt]
  &= - \log p_{\theta}\left(x_0\right) + \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{p_{\theta}\left(x_{0:T}\right)} + \log {p_{\theta}\left(x_0\right)}\right] \\[10pt]
  &= \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{p_{\theta}\left(x_{0:T}\right)}\right]
\end{align}
\\\\
We can subsequently define $L_{VLB}$ as the loss to be minimised:
\begin{gather}
  L_{VLB} = \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{p_{\theta}\left(x_{0:T}\right)}\right] 
\end{gather}
Ho et al. \cite{ho2020denoising} reformulate the loss in order to reduce variance:
\begin{align}
  L_{VLB} &= \mathbb{E}_q \left[\log \frac{q\left(x_{1:T}|x_0\right)}{p_{\theta}\left(x_{0:T}\right)}\right] \\
  &= \mathbb{E}_q \left[\log \frac{\prod_{t \geq 1} q\left(x_t | x_{t-1}\right)}{p\left(x_T\right) \prod_{t \geq 1} p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{\prod_{t \geq 1} q\left(x_t | x_{t-1}\right)}{\prod_{t \geq 1} p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \prod_{t \geq 1} q\left(x_t | x_{t-1}\right) - \log \prod_{t \geq 1} p_{\theta}\left(x_{t-1} | x_t\right)\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \sum_{t \geq 1} \log q\left(x_t | x_{t-1}\right) - \sum_{t \geq 1} \log p_{\theta}\left(x_{t-1} | x_t\right)\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \sum_{t \geq 1} \log q\left(x_t | x_{t-1}\right) - \log p_{\theta}\left(x_{t-1} | x_t\right)\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \sum_{t \geq 1} \log \frac{q\left(x_t | x_{t-1}\right)}{p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{q\left(x_1 | x_0\right)}{p_{\theta}\left(x_0 | x_1\right)} + \sum_{t \geq 2} \log \frac{q\left(x_t | x_{t-1}\right)}{p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{q\left(x_1 | x_0\right)}{p_{\theta}\left(x_0 | x_1\right)} + \sum_{t \geq 2} \log \frac{q\left(x_{t-1} | x_t, x_0\right) q\left(x_t | x_0\right)}{{q\left(x_{t-1} | x_0\right)} p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{q\left(x_1 | x_0\right)}{p_{\theta}\left(x_0 | x_1\right)} + \sum_{t \geq 2} \log \frac{q\left(x_{t-1} | x_t, x_0\right)}{p_{\theta}\left(x_{t-1} | x_t\right)} + \sum_{k \geq 2} \log \frac{q\left(x_t | x_0\right)}{q\left(x_{t-1} | x_0\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{q\left(x_1 | x_0\right)}{p_{\theta}\left(x_0 | x_1\right)} + \log \frac{q\left(x_T | x_0\right)}{q\left(x_1 | x_0\right)} + \sum_{t \geq 2} \log \frac{q\left(x_{t-1} | x_t, x_0\right)}{p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[-\log\left(p_{\theta}\left(x_T\right)\right) + \log \frac{q\left(x_T | x_0\right)}{p_{\theta}\left(x_0 | x_1\right)} + \sum_{t \geq 2} \log \frac{q\left(x_{t-1} | x_t, x_0\right)}{p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \left[- \log\left(p_{\theta}\left(x_0 | x_1\right)\right) + \log \frac{q\left(x_T | x_0\right)}{p_{\theta}\left(x_T\right)} + \sum_{t \geq 2} \log \frac{q\left(x_{t-1} | x_t, x_0\right)}{p_{\theta}\left(x_{t-1} | x_t\right)}\right] \\
  &= \mathbb{E}_q \: \Bigg[ D_{KL}\left(q\left(x_T | x_0\right) \| \: p\left(x_T\right)\right) \Bigg.  \\
  &\qquad \qquad \Bigg. + \sum_{t \geq 2} D_{KL}\left(q\left(x_{t-1} | x_t, x_0\right) \| \: p_{\theta}\left(x_{t-1} | x_t\right)\right) - \log\left(p_{\theta}\left(x_0 | x_1\right)\right) \Bigg] \notag
\end{align}
\\\\
This formulation of the loss can be split up into 3 parts:
\begin{align}
  L_T &= D_{KL}\left(q\left(x_T | x_0\right) \| \: p\left(x_T\right)\right) \\
  L_{1:T-1} &= \sum_{t \geq 2} D_{KL}\left(q\left(x_{t-1} | x_t, x_0\right) \| \: p_{\theta}\left(x_{t-1} | x_t\right)\right) \\
  L_0 &=  - \log\left(p_{\theta}\left(x_0 | x_1\right)\right)
\end{align}
\\
It is worth noting that at equation \ref{eq:22}, the terms of $q$ are conditioned on $x_0$; this is in order to be able to easily compute the forward process posteriors, as they are much easier to find given the original image $x_0$. \\
Ho et al. \cite{ho2020denoising} define it as:
\begin{gather}
  q\left(x_{t-1} | x_t, x_0\right) = \mathcal{N}\left(x_{t-1}; \tilde{\mu}_t \left(x_t, x_0\right), \tilde{\beta_t} I\right)
\end{gather}
\\\\
Using Bayes' theorem, we find:

\begin{align}
  q\left(x_{t-1} | x_t, x_0\right) &= \mathcal{N}\left(x_{t-1}; \tilde{\mu}_t \left(x_t, x_0\right), \tilde{\beta_t} I\right) \\
  &= q\left(x_t | x_{t-1}, x_0\right) \frac{q\left(x_{t-1}| x_0\right)}{q\left(x_t | x_0\right)} \\[10pt]
  &= \mathcal{N}\left(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I\right) \frac{\mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}} x_0, \left(1 - \bar{\alpha}_{t-1}\right)I\right)}{\mathcal{N}\left(\sqrt{\bar{\alpha}_t} x_0, \left(1 - \bar{\alpha}_t\right)I\right)} \\[15pt]
  &= \frac{1}{\sqrt{2 \pi \left(1 - \bar{\alpha}_t\right) \left(1 - \bar{\alpha}_{t-1}\right) \left(\sqrt{1 - \beta_t}\right)}} \notag \\
  &\qquad \qquad e^{- \frac{1}{2} \left(\frac{\left(x_t - \sqrt{1 - \beta_t}x_{t-1}\right)^2}{\beta_t} + \frac{\left(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}}x_0\right)^2}{1 - \bar{\alpha}_{t-1}} - \frac{\left(x_t - \sqrt{\alpha}_t x_0\right)^2}{1 - \bar{\alpha}_{t}}\right)}
\end{align}
\\\\
\textit{Note}: we rewrote $q\left(x_t | x_{t-1}, x_0\right)$ as $q\left(x_t | x_{t-1}\right)$ since $x_0$ adds no new information not already present \indent \indent in $x_{t-1}$.
\\\\
We can ignore the constant factor in front and thus obtain:
\begin{align}
  &= \exp\left(- \frac{1}{2} \left(\frac{x_t^2 - 2 \sqrt{\alpha_t} x_{t-1} x_t + \alpha_t x_{t-1}^2}{\beta_t} \right. \right. \\
  & \hspace{4cm} + \frac{x_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}} x_0 x_{t-1} + \bar{\alpha}_{t-1} x_0^2}{ 1 - \bar{\alpha}_{t-1}}  \notag \\
  & \left. \left. \hspace{8cm} - \frac{\left(x_t - \sqrt{\alpha}_t x_0\right)^2}{1 - \bar{\alpha}_{t}}\right) \right) \notag
\end{align}
\begin{align}
  &= \exp\left(- \frac{1}{2} \left(\frac{x_t^2}{\beta_t} - \frac{2 \sqrt{\alpha_t} x_t x_{t-1}}{\beta_t} + \frac{\alpha_t x_{t-1}^2}{\beta_t} + \frac{x_{t-1}^2}{1 - \bar{\alpha}_{t-1}} \right. \right. \\
  & \left. \left. \hspace{4cm}  - \frac{2 \sqrt{\bar{\alpha}_{t-1}} x_0 x_{t-1}}{1 - \bar{\alpha}_{t-1}} + \frac{\bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}} - \frac{\left(x_t - \sqrt{\alpha}_t x_0\right)^2}{1 - \bar{\alpha}_{t}}\right) \right) \notag \\[10pt]
  &= \exp\left(- \frac{1}{2} \left(x_{t-1}^2 \left(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) + x_{t-1} \left(\frac{-2 \sqrt{\alpha_t} x_t}{\beta_t} - \frac{2 \sqrt{\bar{\alpha}_{t-1}} x_0 }{1 - \bar{\alpha}_{t-1}}\right) + C\left(x_t, x_0\right)\right) \right) 
\end{align}
where:
\begin{gather}
  C\left(x_t, x_0\right) = \frac{x_t^2}{\beta_t} + \frac{\bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}} - \frac{\left(x_t - \sqrt{\alpha}_t x_0\right)^2}{1 - \bar{\alpha}_{t}}
\end{gather}
Considering $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=0}^{t}{\alpha_i}$, we can simplify the expression to:
\begin{align}
  &= \exp\left(- \frac{1}{2} \left(x_{t-1}^2 \left(\frac{\alpha_t}{1 - \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right) \right. \right. \\
  &\hspace{4cm} \left. \left. - 2\left(\frac{\sqrt{\alpha_t} x_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_{t-1}}\right) x_{t-1} + C\left(x_t, x_0\right) \right) \right) \notag \\[10pt]
  &= \exp\left(- \frac{1}{2} \left(x_{t-1}^2 \left(\frac{\alpha_t \left(1 - \bar{\alpha}_{t-1}\right) + \left(1 - \alpha_t\right)}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}\right) \right. \right. \\
  &\hspace{4cm} \left. \left. - 2\left(\frac{\sqrt{\alpha_t} x_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_{t-1}}\right) x_{t-1} + C\left(x_t, x_0\right) \right) \right) \notag \\[10pt]
  &= \exp\left(- \frac{1}{2} \left(x_{t-1}^2 \left(\frac{\alpha_t - \alpha_t \bar{\alpha}_{t-1} + 1 - \alpha_t}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}\right) \right. \right. \\
  &\hspace{4cm} \left. \left. - 2\left(\frac{\sqrt{\alpha_t} x_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_{t-1}}\right) x_{t-1} + C\left(x_t, x_0\right) \right) \right) \notag \\[10pt]
  &= \exp\left(- \frac{1}{2} \left(x_{t-1}^2 \left(\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}\right) \right. \right. \\
  &\hspace{4cm} \left. \left. - 2\left(\frac{\sqrt{\alpha_t} x_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_{t-1}}\right) x_{t-1} + C\left(x_t, x_0\right) \right) \right) \notag \\[10pt]
  &= \exp\left(- \frac{1}{2} \left( \left(\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}\right)  \right. \right. \\
  &\hspace{4cm} \left. \left. \left(x_{t-1}^2 - 2 \frac{\frac{\sqrt{\alpha_t}x_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1 - \bar{\alpha}_{t-1}}}{\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}} x_{t-1} + \frac{C\left(x_t, x_0\right)}{\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}}\right) \right) \right) \notag \\[10pt]
  &= \exp \left(- \frac{1}{2} \left( \left(\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}\right)  \right. \right. \\
  &\hspace{2cm} \left. \left. \left(x_{t-1}^2 - 2 \frac{\sqrt{\alpha_t} \left(1 - \bar{\alpha}_{t-1}\right) x_t + \sqrt{\bar{\alpha}_{t-1}}\left(1 - \alpha_t\right) x_0}{1 - \bar{\alpha}_t} + \frac{C\left(x_t, x_0\right)}{\frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}}\right) \right) \right) \notag 
\end{align}
\\
As for the final term, since:
\begin{align}
  C \left(x_t, x_0\right) &= \frac{x_t^2}{\beta_t} + \frac{\bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}} - \frac{\left(x_t - \sqrt{\alpha}_t x_0\right)^2}{1 - \bar{\alpha}_{t}} \\
  &= \frac{x_t^2}{1 - \alpha_t} + \frac{\bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}} - \frac{x_t^2 - 2 \sqrt{\alpha}_t x_t x_0 + \alpha_t x_0^2}{1 - \bar{\alpha}_{t}}
\end{align}
\\\\
We obtain:
\begin{equation}
  C\left(x_t, x_0\right) / \frac{1 - \bar{\alpha}_{t-1}}{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)} 
\end{equation}
\begin{align}
    &= \frac{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_{t-1}} \left( \frac{x_t^2}{1 - \alpha_t} + \frac{\bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}} - \frac{x_t^2 - 2 \sqrt{\alpha}_t x_t x_0 + \alpha_t x_0^2}{1 - \bar{\alpha}_{t}} \right) \\[15pt]
    &= x_t^2 \left(\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} - \frac{\left(1 - \alpha_t\right) \left(1 - \bar{\alpha}_{t-1}\right)}{\left(1 - \bar{\alpha}_t\right)^2}\right) \\[5pt]
    & \hspace{4cm}+ x_0^2 \left( \frac{\left(1 - \alpha_t\right)\left(\bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_t} - \frac{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)\bar{\alpha}_t}{\left(1 - \bar{\alpha}_t\right)^2} \right) \notag \\[5pt]
    & \hspace{7cm} + \frac{2 x_t x_0 \sqrt{\bar{\alpha}_t}\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{\left(1 - \bar{\alpha}_{t}\right)^2} \notag \\[15pt]
    &= x_t^2 \left(\frac{ \left( 1 - \bar{\alpha}_t \right) \left( 1 - \bar{\alpha}_{t-1} \right) - \left(1 - \alpha_t\right) \left(1 - \bar{\alpha}_{t-1}\right)}{\left(1 - \bar{\alpha}_t\right)^2}\right) \\[5pt]
    & \hspace{4cm} + x_0^2 \left( \frac{ \left( 1 - \bar{\alpha}_t \right) \left( 1 - \alpha_t \right) \bar{\alpha}_{t-1} - \left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)\bar{\alpha}_t}{\left(1 - \bar{\alpha}_t\right)^2} \right) \notag \\[5pt]
    & \hspace{7cm} + \frac{2 x_t x_0 \sqrt{\bar{\alpha}_t}\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{\left(1 - \bar{\alpha}_{t}\right)^2} \notag \\[15pt]
    &= x_t^2 \frac{ \left( 1 - \bar{\alpha}_{t-1} \right)^2 \alpha_t }{\left(1 - \bar{\alpha}_t\right)^2} + x_0^2 \frac{\left( 1 - \bar{\alpha}_{t} \right)^2 \bar{\alpha}_{t-1}}{\left(1 - \bar{\alpha}_t\right)^2} + \frac{2 x_t x_0 \sqrt{\bar{\alpha}_t}\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{\left(1 - \bar{\alpha}_{t}\right)^2} \\[15pt]
    &= \frac{\left( x_t \sqrt{\alpha_t} \left( 1 - \bar{\alpha}_{t-1} \right) + x_0 \sqrt{\bar{\alpha}_{t-1}} \left( 1 - \bar{\alpha}_t \right) \right)^2}{\left(1 - \bar{\alpha}_{t}\right)^2} \\[15pt]
    &= \left(\frac{\left( x_t \sqrt{\alpha_t} \left( 1 - \bar{\alpha}_{t-1} \right) + x_0 \sqrt{\bar{\alpha}_{t-1}} \left( 1 - \bar{\alpha}_t \right) \right)}{\left(1 - \bar{\alpha}_{t}\right)} \right)^2
\end{align}
We can now replace this value into the original expression:
\begin{align}
  q(x_{t-1} | x_t, x_0) &= \exp \left(- \frac{1}{2} \left( \left(\frac{1}{\frac{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_{t-1}}}\right) \right. \right. \\[5pt]
  & \hspace{2cm} \left(x_{t-1}^2 -2 \frac{\sqrt{\alpha_t} \left(1 - \bar{\alpha}_{t-1}\right) x_t + \sqrt{\bar{\alpha}_{t-1}}\left(1 - \alpha_t\right) x_0}{1 - \bar{\alpha}_t} \right. \notag \\[5pt]
  & \hspace{3cm} \left. \left. \left. + \left(\frac{\left( x_t \sqrt{\alpha_t} \left( 1 - \bar{\alpha}_{t-1} \right) + x_0 \sqrt{\bar{\alpha}_{t-1}} \left( 1 - \bar{\alpha}_t \right) \right)}{\left(1 - \bar{\alpha}_{t}\right)} \right)^2 \right) \right) \right) \\[15pt]
  &= \exp \left(- \frac{1}{2} \left(\frac{1}{\frac{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_{t-1}}}\right) \right. \\
  & \hspace{3cm} \left. \left( x_{t-1} - \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}}(1 - \alpha_t) x_0}{1 - \bar{\alpha}_t} \right)^2  \right) \notag \\[15pt]
  &= \mathcal{N} \left(x_{t-1}; \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}}(1 - \alpha_t) x_0}{1 - \bar{\alpha}_t} , \frac{\left(1 - \alpha_t\right)\left(1 - \bar{\alpha}_{t-1}\right)}{1 - \bar{\alpha}_{t-1}} I \right) \\[15pt]
  &= \mathcal{N} \left(x_{t-1}; \tilde{\mu}(x_t, x_0), \tilde{\beta}_t I \right)
\end{align}
{
\allowdisplaybreaks \\
We have therefore obtained $\tilde{\mu}(x_t, x_0)$ and $\tilde{\beta}_t$ mentioned in Ho et al.'s \cite{ho2020denoising} paper. \\
The authors also rearrange $\tilde{\mu}(x_t, x_0)$ in order to remove the dependency on $x_0$: \\
  \begin{align}
    x_t &= \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon} \\
    \Leftrightarrow x_0 &= \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon}}{\sqrt{\bar{\alpha}_t}} \\
    \Rightarrow \tilde{\mu}(x_t, t) &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}}(1 - \alpha_t) \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon}}{\sqrt{\bar{\alpha}_t}}}{1 - \bar{\alpha}_t} \\
    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + (1 - \alpha_t) \left(x_t - \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon} \right) \frac{\sqrt{\bar{\alpha}_{t-1}}}{\sqrt{\bar{\alpha}_t}}}{1 - \bar{\alpha}_t} \\
    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t}{{1 - \bar{\alpha}_t}} + \frac{(1 - \alpha_t) \left(x_t - \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon} \right) \frac{1}{\sqrt{\alpha_t}}}{1 - \bar{\alpha}_t} \\
    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{{1 - \bar{\alpha}_t}} x_t + \frac{(1 - \alpha_t)}{(1 - \bar{\alpha}_t) \sqrt{\alpha_t}} x_t - \frac{(1 - \alpha_t)(\sqrt{1 - \bar{\alpha}_t})}{(1 - \bar{\alpha}_t)\sqrt{\alpha_t}} \bar{\epsilon} \\
    &= \left( \frac{\sqrt{\alpha_t} \sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{\sqrt{\alpha_t} ({1 - \bar{\alpha}_t})}+ \frac{(1 - \alpha_t)}{(1 - \bar{\alpha}_t) \sqrt{\alpha_t}} \right) x_t - \frac{(1 - \alpha_t)(\sqrt{1 - \bar{\alpha}_t})}{\sqrt{(1 - \bar{\alpha}_t)}\sqrt{(1 - \bar{\alpha}_t)}\sqrt{\alpha_t}} \bar{\epsilon} \\
    &= \left( \frac{\alpha_t (1 - \bar{\alpha}_{t-1}) + (1 - \alpha_t)}{\sqrt{\alpha_t} ({1 - \bar{\alpha}_t})} \right) x_t - \frac{(1 - \alpha_t)}{\sqrt{(1 - \bar{\alpha}_t)}\sqrt{\alpha_t}} \bar{\epsilon} \\
    &= \left( \frac{\alpha_t - \bar{\alpha}_{t} + 1 - \alpha_t}{\sqrt{\alpha_t} ({1 - \bar{\alpha}_t})} \right) x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}\sqrt{\alpha_t}} \bar{\epsilon} \\
    &= \frac{1}{\sqrt{\alpha_t}} x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}\sqrt{\alpha_t}} \bar{\epsilon} \\
    &= \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}} \bar{\epsilon} \right)
  \end{align}
}
\\\\
Another important point is that $L_T$ can be omitted since $q$ has no learnable parameters and with the fact that it will almost become a Gaussian distribution, the $D_{KL}\left(q\left(x_T | x_0\right) \| p\left(x_T\right)\right)$ (the Kullback-Leibler Divergence between a nearly Gaussian distribution and a Gaussian distribution $p\left(x_T\right) = \mathcal{N}\left(x_t; 0, I\right)$) will be close to 0. \cite{ho2020denoising} \\
We thus find ourselves with:
\begin{gather}
  L_{VLB} = \mathbb{E}_q \left[ \sum_{t \geq 2} D_{KL}\left(q\left(x_{t-1} | x_t, x_0\right) \| p_{\theta}\left(x_{t-1} | x_t\right)\right) - \log\left(p_{\theta}\left(x_0 | x_1\right)\right) \right]
\end{gather}

\subsubsection{Simplified training objective}

Ho et al. \cite{ho2020denoising} simplify the training objective by making several changes in order to obtain an easy quantity for minimising the loss. 
\\\\
First of all, $L_{1: T-1}$ can be written as such:
\begin{align}
  L_{1:T-1} &= D_{KL}\left(q\left(x_{t-1} | x_t, x_0\right) \| \: p_{\theta}\left(x_{t-1} | x_t\right)\right) \\
  &= D_{KL}\left(\mathcal{N} (x_{t-1}; \tilde{\mu}_t \left(x_t, t \right), \sigma^2_t I ) \| \: \mathcal{N} \left( x_{t-1} ; \mu_{\theta}(x_t, t), \sigma^2_t I  \right) \right)
\end{align}
{
  \allowdisplaybreaks
  The Kullback-Leibler divergence between the two multivariate Gaussian distributions is given by:
  \begin{align}
    & D_{KL}\left(\mathcal{N} (x_{t-1}; \tilde{\mu}_t \left(x_t, t \right), \sigma^2_t I ) \| \: \mathcal{N} \left( x_{t-1} ; \mu_{\theta}(x_t, t), \sigma^2_t I  \right) \right) \\
    &= \frac{1}{2} \left( tr\left( (\sigma^2_t I)^{-1} (\sigma^2_t I) \right) - d + \left( \mu_{\theta} - \tilde{\mu}_t \right)^T (\sigma^2_t I)^{-1} \left( \mu_{\theta} - \tilde{\mu}_t \right) + \log \frac{\det(\sigma^2_t I)}{\det(\sigma^2_t I)} \right) \\
    &= \frac{1}{2} \left( d - d + \left( \mu_{\theta} - \tilde{\mu}_t \right)^T (\sigma^2_t I)^{-1} \left( \mu_{\theta} - \tilde{\mu}_t \right) + \log 1 \right) \\
    &= \frac{1}{2} \left( \left( \mu_{\theta} - \tilde{\mu}_t \right)^T (\sigma^2_t I)^{-1} \left( \mu_{\theta} - \tilde{\mu}_t \right) \right) \\
    &= \frac{1}{2 \sigma^2_t} \left( \left( \mu_{\theta} - \tilde{\mu}_t \right)^T \left( \mu_{\theta} - \tilde{\mu}_t \right) \right) \\
    &= \frac{1}{2 \sigma^2_t} \| \mu_{\theta} - \tilde{\mu}_t \|_2^2
  \end{align}
  \\
  However, Ho et al. \cite{ho2020denoising} rewrite this formula so that it depends only on the noise $\epsilon$:
  \begin{align}
    \frac{1}{2 \sigma^2_t} \| \mu_{\theta} - \tilde{\mu}_t \|_2^2 &= \frac{1}{2 \sigma^2_t} \left\| \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}} \bar{\epsilon} \right) - \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}} \epsilon_\theta \right)\right\|_2^2 \\
    &= \frac{1}{2 \sigma^2_t} \left\| \frac{1}{\sqrt{\alpha_t}} \frac{1 - \alpha_t}{\sqrt{(1 - \bar{\alpha}_t)}} (\bar{\epsilon} - \epsilon_\theta) \right\|_2^2 \\
    &= \frac{1}{2 \sigma^2_t} \frac{1}{\alpha_t} \frac{(1 - \alpha_t)^2}{1 - \bar{\alpha}_t} \| \bar{\epsilon} - \epsilon_\theta \|_2^2 \\
    &= \frac{\beta_t^2}{2 \sigma^2_t \alpha_t (1 - \bar{\alpha}_t)} \| \bar{\epsilon} - \epsilon_\theta \|_2^2
  \end{align}
}
Furthermore, they found that simply ignoring the factor in front gave better results, giving us:
\begin{equation}
  L_{1:T-1} = \| \bar{\epsilon} - \epsilon_\theta \|_2^2
\end{equation}
\\\\
As for the second term $L_0$, the authors defined $p_\theta(x_0 | x_1)$ as:
\begin{align}
  p_\theta(x_0 | x_1) &= \prod_{i=1}^{D} \int_{\delta_{-} (x_0^i)}^{\delta_{+} (x_0^i)} \mathcal{N}(x; \mu_\theta^i (x_1, 1), \sigma^2_1) \: dx
\end{align}
\hspace{0.5cm} where $\delta_{+} (x)$ and $\delta_{-} (x)$ are defined as:
\begin{align}
  \delta_{+} (x) &= \begin{cases}
    \infty & \text{if } x = 1 \\
    x + \frac{1}{255} & \text{if } x < 1 
  \end{cases}\\
  \delta_{-} (x) &= \begin{cases}
    - \infty & \text{if } x = - 1 \\
    x - \frac{1}{255} & \text{if } x > -1 
  \end{cases}
\end{align}
\hspace{0.5cm} with $D$ the data dimensionality and $i$ one coordinate from the image.
\\\\
This is a decoder for the last step in order to get pixel values between $\{0, 1, ..., 255 \}$ from the values in $[-1, 1]$ used for the neural network \cite{ho2020denoising,nichol2021improved} \\
However the authors also ignored the entire term, giving us:
\begin{equation}
  L_{\text{simple}}(\theta) = \mathbb{E}_q \left[ \| \bar{\epsilon} - \epsilon_\theta \|_2^2 \right]
\end{equation}
\subsubsection{Model architecture}
Ho et al. \cite{ho2020denoising} as well as the following papers use a U-Net style architecture for the neural network to estimate the noise.
\\\\
The U-Net architecture is a convolutional neural network (CNN) that was originally designed for image segmentation in biomedical imaging \cite{ronneberger2015unet}. \\
It consists of an encoder (or contracting path) and a decoder (or expansive path) that are connected by a bottleneck. \\
The encoder takes in the noisy image as input and repeatedly applies unpadded convolutions followed by max pooling layers in order to reduce the spatial dimensions of the image. \cite{ronneberger2015unet} \\
The bottleneck contains 3 unpadded convolutions each followed by a rectified linear unit (ReLU) activation function. \cite{ronneberger2015unet} \\
The decoder then applies up-convolutions followed by unpadded convolutions to increase the spatial dimensions of the image, with at each step a concatenation of the feature maps from the encoder. \cite{ronneberger2015unet}
\begin{figure}[h]
  \includegraphics[width=\textwidth]{images/unet.png}
  \caption{Original U-Net architecture \cite{ronneberger2015unet}}
\end{figure}
\\\\
However, Ho et al. \cite{ho2020denoising} made some changes to the original U-Net architecture. \\
First, they state that they used 4 feature map resolutions for their $32 \times 32$ resolution model, and 6 for the $256 \times 256$ resolution model. \\
For each of these feature map, they used two convolutional residual blocks similar of those of a Wide ResNet \cite{zagoruyko2017wide} consisting of a convolutional layer, a ReLU activation function, a convolutional layer, a group normalisation layer, and another ReLU activation function. \\ The "residual" part of the block comes from the fact that the result at the end of the block on the encoder side is concatenated with the result at the end of the block on the decoder side in order to consider features that could have been lost during the downsampling. \cite{lai2022rethinking} \\
They used group normalisation \cite{wu2018group} rather than weight normalisation \cite{salimans2016weight}, which is a normalisation technique that divides the channels into groups and computes the mean and standard deviation for each group \cite{ho2020denoising}. \\
Finally, they state that they used self-attention layers from the Transformer architecture \cite{vaswani2023attention} at the $16 \times 16$ resolution between the two convolutional residual blocks. \cite{ho2020denoising}
\\\\
Since the model needs to be able to predict the noise at any timestep $t$, the authors added a time embedding to the input of the network using the sinusoidal positional embedding from the Transformer architecture \cite{vaswani2023attention}, which allows the parameters to be shared across all timesteps. \\
The sinusoidal positional embedding is given by \cite{vaswani2023attention}:
\begin{align}
  PE_{(pos, 2i)} &= \sin \left( \frac{pos}{10000^{2i/d_{model}}} \right) \\
  PE_{(pos, 2i+1)} &= \cos \left( \frac{pos}{10000^{2i/d_{model}}} \right) \\
  \text{where } \qquad & pos \text{ is the position and } i \text{ is the dimension} \notag
\end{align}





\subsection{Comparison with other diffusion models}

\newpage
\section{Improvements upon DDPMs}

\newpage
\section{Results}

\newpage
\section{Conclusion}

\newpage
\appendix
\section{Convolution of two Gaussian distributions}
Equation \ref{eq:} had given us:
\begin{align*}
  x_t &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \alpha_t\left(1 - \alpha_{t-1}\right)\right) + \mathcal{N}\left(0, 1 - \alpha_t\right)
\end{align*}
We can find the sum of two independant random variables $X \sim \mathcal{N}\left(\mu_X, \sigma_X^2\right)$ and $Y \sim \mathcal{N}\left(\mu_Y, \sigma_Y^2\right)$ by finding the convolution of two Gaussian distributions.
\\\\
Let $Z = X + Y$ with $X$ and $Y$ independent and $\sigma_Z = \sqrt{\sigma_X^2 + \sigma_Y^2}$, the convolution of two Gaussian distributions is given by:
{
  \allowdisplaybreaks
\begin{align*}
  f_Z(z) &= \int_{-\infty}^{\infty} f_X(x) f_Y(z - x) \: dx \\
  &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma_X^2}} \exp \left[{-\frac{(x - \mu_X)^2}{2 \sigma_X^2}} \right] \frac{1}{\sqrt{2 \pi \sigma_Y^2}} \exp \left[{-\frac{(z - x - \mu_Y)^2}{2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp \left[{-\frac{(x - \mu_X)^2}{2 \sigma_X^2}} \right] \exp \left[{-\frac{(z - x - \mu_Y)^2}{2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp\left[{-\frac{(x - \mu_X)^2}{2 \sigma_X^2} - \frac{(z - x - \mu_Y)^2}{2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp\left[{-\frac{x^2 - 2 \mu_X x + \mu_X^2}{2 \sigma_X^2} - \frac{z^2 + x^2 - 2 x z - 2 z \mu_Y + 2 \mu_Y x + \mu_Y^2}{2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp\left[{-\frac{\sigma_Y^2(x^2 - 2 \mu_X x + \mu_X^2)}{2 \sigma_X^2 \sigma_Y^2} - \frac{\sigma_X^2(z^2 + x^2 - 2 x z - 2 z \mu_Y + 2 \mu_Y x + \mu_Y^2)}{2 \sigma_X^2 \sigma_Y^2 }} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp\left[{-\frac{\sigma_Y^2 x^2 - 2 \mu_X \sigma_Y^2 x + \mu_X^2 \sigma_Y^2 + \sigma_X^2 z^2 + \sigma_X^2 x^2 - 2xz\sigma_X^2 + 2 z \mu_Y \sigma_X^2 - 2\mu_Y x \sigma_X^2 + \sigma_X^2 \mu_Y^2}{2 \sigma_X^2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{2 \pi \sigma_X \sigma_Y} \int_{-\infty}^{\infty} \exp\left[{-\frac{\left(\sigma_X^2 + \sigma_Y^2\right) x^2 - 2 x \left( \mu_X \sigma_Y^2 + z \sigma_X^2 - \mu_Y \sigma_X^2 \right) + \sigma_X^2 \left( z - \mu_Y \right)^2 + \mu_X^2 \sigma_Y^2}{2 \sigma_X^2 \sigma_Y^2}} \right] \: dx \\[10pt]
  &= \frac{1}{\sqrt{2 \pi} \frac{\sigma_X \sigma_Y}{\sigma_Z}} \frac{1}{\sqrt{2 \pi} \sigma_Z} \int_{-\infty}^{\infty} \exp\left[{-\frac{x^2 - 2x \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2} + \frac{\sigma_X^2 \left( z - \mu_Y \right)^2 + \mu_X^2 \sigma_Y^2}{\sigma_Z^2}}{2 \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \right] \: dx \\[10pt]
  &= \frac{1}{\sqrt{2 \pi} \frac{\sigma_X \sigma_Y}{\sigma_Z}} \frac{1}{\sqrt{2 \pi} \sigma_Z} \int_{-\infty}^{\infty} \exp\left[{-\frac{\left( x - \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2}\right)^2 - \left( \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2} \right)^2 + \frac{\sigma_X^2 (z - \mu_Y)^2 + \sigma_Y^2 \mu_X^2}{\sigma_Z^2}}{2 \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \right] \: dx  \\[10pt]
  &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} \frac{\sigma_X \sigma_Y}{\sigma_Z}} \exp\left[{-\frac{\left( x - \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2}\right)^2}{2 \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \right] \\[10pt]
  & \hspace{3cm} \frac{1}{\sqrt{2 \pi} \sigma_Z} \exp \left[ - \frac{\sigma_Z^2 \left( \sigma_X^2 (z - \mu_Y)^2 + \sigma_Y^2 \mu_X^2 \right) - \left( \sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X \right)^2}{2 \sigma_Z^2 (\sigma_X \sigma_Y)^2} \right] \: dx  \\[10pt]
  &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} \frac{\sigma_X \sigma_Y}{\sigma_Z}} \exp\left[{-\frac{\left( x - \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2}\right)^2}{2 \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \right] \frac{1}{\sqrt{2 \pi} \sigma_Z} \exp \left[ - \frac{(z - (\mu_X + \mu_Y))^2}{2 \sigma_Z^2} \right] \: dx  \\[10pt]
  &= \frac{1}{\sqrt{2 \pi} \sigma_Z} \exp \left[ - \frac{(z - (\mu_X + \mu_Y))^2}{2 \sigma_Z^2} \right] \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} \frac{\sigma_X \sigma_Y}{\sigma_Z}} \exp\left[{-\frac{\left( x - \frac{\sigma_X^2 (z - \mu_Y) + \sigma_Y^2 \mu_X}{\sigma_Z^2}\right)^2}{2 \left(\frac{\sigma_X \sigma_Y}{\sigma_Z}\right)^2}} \right] \: dx  \\[10pt]
  &= \frac{1}{\sqrt{2 \pi} \sigma_Z} \exp \left[ - \frac{(z - (\mu_X + \mu_Y))^2}{2 \sigma_Z^2} \right] \\[10pt]
  &= \mathcal{N}\left(z; \mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2 \right)
\end{align*}
}
\\
Given this, we can refer back to our case and therefore we find:
\begin{align*}
  x_t &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \alpha_t\left(1 - \alpha_{t-1}\right)\right) + \mathcal{N}\left(0, 1 - \alpha_t\right) \\
  &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \mathcal{N}\left(0, \alpha_t\left(1 - \alpha_{t-1}\right) + 1 - \alpha_t\right) 
\end{align*}
%% ----------------------------------------------------------------- %%

\newpage
\printbibliography

\end{document}
