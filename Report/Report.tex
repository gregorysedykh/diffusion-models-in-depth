\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry} % Add geometry package here
\geometry{margin=1.5in} % Set the margin size here
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{biblatex}

\addbibresource{sources.bib}

\pgfplotsset{compat=1.16}

\graphicspath{ {./images/} }

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    pdfpagemode=FullScreen,
}

\definecolor{code-gray}{RGB}{220, 220, 220}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[LE,RO]{\thepage}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  showstringspaces=false,
  backgroundcolor=\color{code-gray}
}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}


\begin{document}

\begin{titlepage}
  \begin{center}
      \vspace*{1cm}

      \LARGE
      \textbf{Diffusion Models in Depth: From a Theoretical and a Practical Perspective}

      \vspace{1.5cm}

      \Large
      \textbf{Gregory Sedykh}
      \vspace{0.8cm}

      \normalsize
      June 28th 2024

      \vfill

      \includegraphics[width=0.4\textwidth]{images/informatics_en.png} \\

      Bachelor thesis for the degree of Bachelor of Science in Computer Science \\
      Supervised by Prof. St√©phane Marchand-Maillet     

      \vspace{0.8cm}
    
           
           
  \end{center}
\end{titlepage}

\pagenumbering{roman}
\newpage
\section*{Abstract}

\newpage
\tableofcontents

\pagenumbering{arabic}

%% ----------------------- Content goes here ----------------------- %%

\newpage
\section{Introduction}

Diffusion models have gained widespread popularity since 2020, as models such as DALL-E, Stable Diffusion and Midjourney have proven to be capable of generating high-quality images given a text prompt. Furthermore, OpenAI's recent announcement of Sora has shown that diffusion models have now also become highly capable of generating minute long high-definition videos from a text prompt. \cite{videoworldsimulators2024}
\\\\
These models date back to 2015, where the idea of a diffusion model appeared, based on diffusion processes used in thermodynamics. \cite{sohldickstein2015deep} \\
Denoising Diffusion Probabilistic Models (DDPMs) were a development of the original diffusion probabilistic model introduced in 2015. \cite{ho2020denoising} \\
Subsequently, OpenAI improved upon the original DDPMs which did not have ideal log likelihoods \cite{ho2020denoising} while also using less forward passes and therefore speeding up the sampling process. \cite{nichol2021improved} \\
The most recent progress done by OpenAI has allowed their diffusion models to obtain better metrics and better sample quality than Generative Adversarial Networks (GANs) which were previously considered the state-of-the-art in image generation. \cite{dhariwal2021diffusion}
\\\\
The fairly recent apparition of diffusion models means not only that there is still a lot to be discovered about them, but also that progress is being made rapidly. \\
The theory behind diffusion models was mainly founded when Ho et al. \cite{ho2020denoising} introduced their DDPMs in 2020, but many improvements have been made upon their work since then. \\ 
To understand what these models are and how they work, it is crucial to understand how DDPMs were developed, what choices were made when developing them and why these choices were made, as well as why changes were made to the original model and how they were made.
\\\\
This report aims to provide an overview of the theory behind diffusion models, as well as a practical guide on how to implement a simple diffusion model using PyTorch, in order to compare what the theory shows us and what the practical implementation gives us.

\newpage
\section{Denoising Diffusion Probabilistic Models}

A \textbf{diffusion model} is a generative model that consists of two Markov chains, one forward and one reverse. \\
Given an input (e.g. an image), the forward process will destroy the image by gradually adding Gaussian noise at each step of the process. \cite{ho2020denoising} \\
The reverse process' objective is to "reverse" the forward process, starting from the noisy image and step-by-step, estimating the noise that was added to the image at each step and removing it until the first step is reached, where we should obtain the original image. \cite{ho2020denoising} \\

\subsection{Forward Process}

The forward process is a Markov process that starts from the original image $x_0$ and adds Gaussian noise during $T$ steps, where each step $i \in [1, T]$ has a size $\beta_i \in \{ \beta_1, ..., \beta_T \}$, which results in a more noisy image $x_i$. \cite{ho2020denoising}
\\\\
Formally, we obtain:
\begin{gather}
  q(x_{1},..., x_{T} | x_0) = \prod_{t = 1}^T{q(x_t | x_{t - 1})} \label{eq:1} \\
  q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I) \label{eq:2}
\end{gather}
\\\\
Equation \ref{eq:2} gives us a single step forward: given the previous image $x_{t-1}$, we add noise from a Gaussian distribution with mean $\sqrt{1 - \beta_t}x_{t-1}$ and variance $\beta_t I$ to obtain the next image $x_t$. \\
Equation \ref{eq:1} gives us the full forward process from the original image $x_0$ to the final image $x_T$.
\\\\
It is important to note that as $T \rightarrow \infty$ and with a correct choice of $\beta_t$,  $x_T$ will become a sample of an isotropic Gaussian distribution ($\mathcal{N}(0, I)$). \cite{nichol2021improved} \cite{sohldickstein2015deep}. \\
This is important for the reverse process, as it will allow us to take a sample $x_T \sim \mathcal{N}(0, I)$ and reverse the forward process to obtain the original image $x_0$ (however this cannot be done so simply, as seen in section 3) \cite{nichol2021improved}.
\\\\
The reparametrisation trick says that for a univariate Gaussian distribution where $z \sim p(z | x) = \mathcal{N}(\mu, \sigma^2)$, a valid reparametrisation would be $z = \mu + \sigma \epsilon$ where $\epsilon \sim \mathcal{N}(0, 1)$ is just noise. \cite{kingma2022autoencoding} \\
Ho et al. \cite{ho2020denoising} use this trick to be able to sample $x_t$ at any arbitrary step $t$ of the forward process.
\\\\
Let $\alpha_t = 1 - \beta_t$, then:
\begin{gather*}
  x_t \sim \mathcal{N}(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I) \\
  x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t I} \epsilon_{t-1} \qquad \epsilon_{t - 1} \sim \mathcal{N}(0, I) \\
  x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t - 1}
\end{gather*}
\\\\
From this, we can apply it again to $x_{t-1}$ and obtain:
\begin{gather*}
  x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t - 2}
\end{gather*}
\\\\
Therefore, we get:
\begin{gather*}
  x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})} \epsilon_{t - 2} + \sqrt{1 - \alpha_t} \epsilon_{t - 1} \label{eq:8}
\end{gather*}
\\\\
Using the reparametrisation trick again, we can write:
$$\sqrt{\alpha_t(1 - \alpha_{t-1})} \epsilon_{t - 2}$$ 
as 
$$w \sim \mathcal{N}(0, \alpha_t(1 - \alpha_{t-1}))$$
and
$$\sqrt{1 - \alpha_t} \epsilon_{t - 1}$$
as
$$z \sim \mathcal{N}(0, 1 - \alpha_t)$$
\\\\
Since the sum of two Gaussian distributions is also a Gaussian distribution, we can write:
$$w + z \sim \mathcal{N}(0, (\alpha_t(1 - \alpha_{t-1}) + 1 - \alpha_t)I)$$
\\\\
Which finally gives us:
\begin{gather*}
  x_t = \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\epsilon}_{t - 2}
\end{gather*}
\\\\
We can recursively apply this until $x_0$. \\
Let $\bar{\alpha}_t = \prod_{i=0}^{t}{\alpha_i}$:
\begin{gather}
  x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon} \label{eq:3}
\end{gather}
\\\\
This will give us a way to find a noisy image at step $t$ of the forward process, given the original image $x_0$ (we ignore the noise $\bar{\epsilon} \sim \mathcal{N}(0, I)$):
\begin{gather}
  q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t)I) \label{eq:4}
\end{gather}
\\
We now also know that $1 - \bar{\alpha}_t$ is the variance of the noise added at step $t$ of the forward process. \cite{nichol2021improved}
\\\\
Choosing the variances $\beta_t$ is an important step when developing the forward process. \\
Ho et al. \cite{ho2020denoising} chose to use a linear schedule starting from $\beta_1 = 10^{-4}$ and increasing linearly until $\beta_T = 0.02$ in order for them to be small compared to the pixel values that were in $[-1, 1]$. \cite{ho2020denoising} \\
However we will see in section 3 that this is not the best choice, as this destroyed the images too quickly closer to the end of the process while providing little to sample quality, which made the model be sub-optimal for $64 \times 64$ and $32 \times 32$ images. \cite{nichol2021improved}
\\\\
Finally, Ho et al. \cite{ho2020denoising} chose $T = 1000$ in order to be able to compare their model with Sohl et al.'s \cite{sohldickstein2015deep} model, which also used $T = 1000$ for most image experiments. \\
We will again see in section 3 that so many steps can make the sampling slow and that there are better choices. \cite{nichol2021improved}
\subsection{Reverse Process}

As mentioned previously, with a correct choice of $\beta_t$, $x_T$ will become a sample of an isotropic Gaussian distribution as $T \rightarrow \infty$. \cite{nichol2021improved, sohldickstein2015deep} \\
This should mean that we can take a sample $x_T \sim \mathcal{N}(0, I)$ and reverse the forward process to obtain the original image $x_0$. 
However, this means that we need to use the entire dataset in order to figure out $q(x_{t-1} | x_t)$, which in practice cannot be done. \cite{nichol2021improved} \\
Therefore, an estimation is found using a neural network. \cite{nichol2021improved}
\\\\
The reverse process is defined as follows \cite{ho2020denoising}:
\begin{gather}
  p_{\theta}(x_0, ..., x_T) = p(x_T) \: \prod_{t=1}^T p_{\theta}(x_{t-1} | x_t) \label{eq:5} \\
  p_{\theta}(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)) \label{eq:6}
\end{gather}
\\\\
Equation \ref{eq:5} gives us the full reverse process, starting from the final image $x_T$ to the original image $x_0$. \\
Equation \ref{eq:6} gives us the reverse process at step $t$, where we estimate the noise that was added to the image at step $t$ of the forward process in order to find the image $x_{t-1}$. \\
Two parameters must be estimated to find the reverse process at step $t$: the mean $\mu_{\theta}(x_t, t)$ and the variance $\Sigma_{\theta}(x_t, t)$. \\
At first, Ho et al. \cite{ho2020denoising} used neural networks to estimate both the mean and the variance, but explain that estimating the variance was not necessary as there was little difference between the estimated variance $\sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}$ and the actual values $\sigma_t^2 = \beta_t$ \cite{ho2020denoising}. Thus they set $\Sigma_{\theta}(x_t, t) = \sigma_t^2 I$. \\
As for the mean, it is estimated using a neural network trained to optimise a variational lower bound ($VLB$) of the negative log likelihood $\mathbb{E}[- \log p_{\theta} (x_0)]$ \cite{ho2020denoising}

\subsubsection{Variational Lower Bound}
In order to find the negative log likelihood $\mathbb{E}[- \log p_{\theta} (x_0)]$, we would need to know $p_{\theta} (x_0)$ which means going through $T$ steps, which is computationally expensive. \\
Subsequently, Ho et al. \cite{ho2020denoising} use a variational lower bound ($VLB$) to estimate the negative log likelihood \cite{ho2020denoising,sohldickstein2015deep}.
The variational lower bound given by Dederik et al. \cite{kingma2022autoencoding} leads us to:



\subsubsection{Simplified training objective}

\subsubsection{U-Net}


\subsection{Comparison with other diffusion models}

\newpage
\section{Improvements upon DDPMs}

\newpage
\section{Results}

\newpage
\section{Conclusion}

%% ----------------------------------------------------------------- %%

\newpage
\printbibliography

\end{document}
